{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_text_generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doERTqn5Ld0u",
        "colab_type": "text"
      },
      "source": [
        "### 예제. RNN을 이용한 텍스트 생성\n",
        "\n",
        "<p>\n",
        " Andrej Karpathy의 Referential Neural Networks의 The Unreasonable Effectiveness에서 셰익스피어가 쓴 글을 데이터 셋으로 이용하겠습니다. 문자 시퀀스 ( \"Shakespear\")가 주어지면 시퀀스의 다음 문자 ( \"e\")를 예측하기 위해 모델을 훈련시킵니다. 모델을 반복적으로 호출하여 더 긴 텍스트 시퀀스를 생성 할 수 있습니다.</p>\n",
        " \n",
        " <p>\n",
        "이 튜토리얼에는 tf.keras와 eager excetuttion을 사용하여 구현하도록 하겠습니다. 일반적인 tensorflow의 연산이 나중에 실행하기 위해 계산 그래프를 만드는 반면, eager execution은 그래프를 작성하지 않고 즉시 평가하여 구체적인 값을 반환합니다. 이를 통해 TensorFlow 및 디버그 모델을 쉽게 시작할 수 있습니다. </p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIANWMk-LHAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhHWqdKJQE3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "a5f09fd6-3ee8-418f-a904-438076def802"
      },
      "source": [
        "# 탄력적으로 GPU memory 사용 방법\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.InteractiveSession(config=config)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d647afd6edef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3upTzRqqLN-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋 다운로드\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_dBbApzL2mz",
        "colab_type": "text"
      },
      "source": [
        "### 데이터 살펴 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB9L_JoKLuBy",
        "colab_type": "code",
        "outputId": "a0f48da6-1da6-4cd9-af9f-f377170a80a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Vg2-o0Lzyd",
        "colab_type": "code",
        "outputId": "f5de74af-7871-48d8-d410-1d27d629c209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# 첫 250자 살펴보기\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo1s3JJjL1jr",
        "colab_type": "code",
        "outputId": "eccc1281-e1e6-49a1-845e-bef96f678955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 고유한 문자들의 수 \n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf8HD6fgMBdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyGHOMmMFbJ",
        "colab_type": "text"
      },
      "source": [
        "### text 처리하기\n",
        "#### text를 벡터화 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uOm7p32MKK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문자->index 맵핑을 만듭니다. \n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuZQfWxMMQ4b",
        "colab_type": "code",
        "outputId": "43239ab5-bb45-4032-fc90-0279b69e869e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZz1qPwXMSQA",
        "colab_type": "code",
        "outputId": "d69db642-3707-482f-8780-b24e96834442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 첫 13개 글자를 맵핑시키면 \n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mhAA8JtMuRL",
        "colab_type": "text"
      },
      "source": [
        "### 문자열이 주어졌을 때 다음에 어떤 문자가 올까요? \n",
        "\n",
        "입력으로 문자열을 받아 다음에 올 문자를 예측하는 모델을 학습시켜 보도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js5ZC99PMVVo",
        "colab_type": "code",
        "outputId": "1b18abde-dacb-4cca-9a1b-65d4ae744777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# 문자열의 최대 길이 지정 \n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# training example 과 target 생성\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCl7SsuhNAtY",
        "colab_type": "code",
        "outputId": "0d403d9d-73f6-4cbc-fc32-6693ddf90fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# batch 를 이용하면 개별 문자를 원하는 크기의 시퀀스로 쉽게 변환 할 수 있습니다.\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHttwZFoNORj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 각 시퀀스마다, map 함수를 이용하여 각 배치에 split_input_target 함수를 적용하여 텍스트를 복제 및 이동합니다.\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFSxFE2WNnNY",
        "colab_type": "code",
        "outputId": "a73451ae-8686-4fbd-9274-8c898018b5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtgLf0EgNpqe",
        "colab_type": "code",
        "outputId": "9247e586-3da5-4b09-f635-50ecc0a05bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "\"\"\"\n",
        "이들 벡터의 각 index는 하나의 시간 간격으로 처리됩니다.\n",
        "시간 단계 0에서 입력에 대해 모델은 \"F\"에 대한 index를 받아 \"i\"에 대한 index를 다음 문자로 예측하려고 시도합니다.\n",
        "다음 timestep에서는 동일한 작업을 수행하지만 RNN은 현재 입력 문자 이외에 이전 단계 문맥(context)를 고려합니다.\n",
        "\"\"\"\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3PS7SAeNru4",
        "colab_type": "code",
        "outputId": "584e52d9-c467-46a0-fa37-f85c8f524067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 배치 사이즈 설정 \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# 데이터셋을 섞을 Buffer size 설정 \n",
        "# (TF 데이터는 infinite 시퀀스와 함께 작동하도록 설계 되었기 때문에 \n",
        "# 전체 시퀀스를 메모리에 셔플하지 않고 데이터를 섞을 버퍼를 관리합니다.\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUnJz4l2PGPD",
        "colab_type": "text"
      },
      "source": [
        "### 모델 생성\n",
        "\n",
        "tf.keras.Sequential를 사용하여 모델을 정의하고 세 개의 레이어를 이 예제에서 사용합니다. \n",
        "\n",
        "- tf.keras.layers.Embedding : 입력 레이어. embedding_dim 차원이있는 벡터에 각 문자의 숫자를 매핑하는 학습 가능한 룩업 테이블입니다. <br>\n",
        "- tf.keras.layers.GRU : 사이즈가 units인 RNN의 한 종류 (여기에서 LSTM 계층을 사용할 수도 있습니다.) <br>\n",
        "- tf.keras.layers.Dense : 출력 레이어. 출력은 vocab_size입니다. <br>\n",
        "\n",
        "#### Simple RNN\n",
        "<img src=\"https://miro.medium.com/max/306/1*28XR1ajfW1WuTOkjpOc9xA.png\" width=200>\n",
        "\n",
        "#### Gated Recurrent Unit (GRU)\n",
        "<img src=\"https://miro.medium.com/max/436/1*GSZ0ZQZPvcWmTVatAeOiIw.png\" width=250>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBjReTFLPDzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 단어 개수\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 문자열을 임베딩할 dimension의 수를 정합니다.\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 유닛의 개수 \n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z47SjJf5PVnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU를 사용가능하다면 CuDNNGRU를 사용합니다. \n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wUaH9idRdGA",
        "colab_type": "text"
      },
      "source": [
        "각 문자에 대해 모델은 임베딩을 찾아, 임베딩을 입력으로하여 GRU 하나의 타임 스텝을 실행합니다. dense 레이어를 적용하여 다음 문자의 log-likelihood을 예측한 logits를 생성합니다.\n",
        "\n",
        "<img src=\"https://tensorflow.org/tutorials/sequences/images/text_generation_training.png\" height=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSqOvyoQQe2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14xH3ktuQnQT",
        "colab_type": "code",
        "outputId": "7ed5e05f-22e6-4123-b2a2-0774f48949cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0718 00:02:22.574079 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
            "W0718 00:02:22.575329 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer\n",
            "W0718 00:02:22.579395 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.global_step\n",
            "W0718 00:02:22.580697 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta1_power\n",
            "W0718 00:02:22.583320 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta2_power\n",
            "W0718 00:02:22.585700 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "W0718 00:02:22.589098 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
            "W0718 00:02:22.591246 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0718 00:02:22.593522 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "W0718 00:02:22.595664 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "W0718 00:02:22.598049 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "W0718 00:02:22.600092 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "W0718 00:02:22.602709 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "W0718 00:02:22.605623 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0718 00:02:22.607303 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "W0718 00:02:22.608494 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "W0718 00:02:22.609934 140052706756480 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "W0718 00:02:22.611207 140052706756480 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXRrJ3UMRqnA",
        "colab_type": "text"
      },
      "source": [
        "### 모델 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y_VUZcuRXTN",
        "colab_type": "code",
        "outputId": "7ba6c403-a8ba-4a8e-cbce-40348d13bc4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzrfFFTRtDa",
        "colab_type": "code",
        "outputId": "199e22b3-84d1-428f-96ca-89cf6d6b2f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_6 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9U4W3GDRviV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 첫번째 example에 대해 시도\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vf9ClsIR9S2",
        "colab_type": "code",
        "outputId": "5a445126-9bc4-47a8-ad98-88c0aa2a42fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21, 16, 47, 53,  9, 24, 58, 56,  8, 24, 37, 34, 13, 62, 20, 62, 60,\n",
              "        8, 33, 11,  8, 27, 21, 16, 47, 42, 64, 49,  7,  7, 10, 59, 33, 36,\n",
              "       38, 17, 40, 58,  3, 27, 12,  9,  5, 45, 50, 49,  6, 11, 29, 62, 36,\n",
              "        3, 47, 32, 42, 54, 45, 60, 10,  7, 21, 52, 24, 41, 27, 42, 46, 57,\n",
              "       37,  6, 64, 31, 48, 19, 15, 16, 16, 28,  7, 49, 12,  5, 50, 21, 56,\n",
              "       21, 62, 17, 38, 33,  2, 64, 47, 33, 16, 22,  6, 30, 22, 39])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_12gLn8sR-Qy",
        "colab_type": "code",
        "outputId": "2bdeb431-0b78-414b-fb48-af3d652ec3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# 디코드\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'll you be gone?\\n\\nVIRGILIA:\\n\\nSICINIUS:\\nAre you mankind?\\n\\nVOLUMNIA:\\nAy, fool; is that a shame? Note bu'\n",
            "\n",
            "Next Char Predictions: \n",
            " \"IDio3Ltr.LYVAxHxv.U;.OIDidzk--:uUXZEbt$O?3'glk,;QxX$iTdpgv:-InLcOdhsY,zSjGCDDP-k?'lIrIxEZU!ziUDJ,RJa\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyGGl52JSGBf",
        "colab_type": "text"
      },
      "source": [
        "### 모델 학습하기\n",
        "이전 RNN state과 현재 인풋이 주어졌을 때 다음에 올 문자의 클래스를 예측합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bItPf5IJSAkL",
        "colab_type": "code",
        "outputId": "2f774783-8a05-4409-f074-0c804645d24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 모델이 logit를 반환하기를 원하기 때문에 from_logits를 설정합니다. \n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1746006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVzro-slSTYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer로 Adam을 설정합니다.\n",
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EubOQMv1Seh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 학습 동안의 모델을 저장할 checkpoint를 지정합니다. \n",
        "# checkpoint가 저장될 폴더\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# checkpoint 파일의 이름\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt8EPePqSqqx",
        "colab_type": "code",
        "outputId": "68135949-dfde-4ed8-fcfe-f3d678c92a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# 학습 실행 \n",
        "EPOCHS=3\n",
        "\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "174/174 [==============================] - 26s 147ms/step - loss: 2.3087\n",
            "Epoch 2/3\n",
            "174/174 [==============================] - 24s 136ms/step - loss: 1.8610\n",
            "Epoch 3/3\n",
            "174/174 [==============================] - 24s 137ms/step - loss: 1.6294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-wfMLeY0LE",
        "colab_type": "code",
        "outputId": "af15f96e-9cf6-4163-8e1a-1ae661a1e825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 18 00:04:03 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    67W / 149W |   1246MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdSdMQtxTOYz",
        "colab_type": "text"
      },
      "source": [
        "### 텍스트 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeLqlEVrSuq2",
        "colab_type": "code",
        "outputId": "f62fa289-5102-4f93-b1b7-3654a3934572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 마지막 checkpoint 불러오기\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs7mH5HQTQvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-obclD4yTSxO",
        "colab_type": "code",
        "outputId": "6f0aa01f-d7a4-4ef6-ae32-d94a5a18b911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_7 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVr2N-jcTS-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMwj6YWUSWk",
        "colab_type": "text"
      },
      "source": [
        "1. 시작 문자열을 선택하고, RNN state 초기화하고 생성 할 문자 수 설정합니다.\n",
        "2. 시작 문자열과 RNN state을 이용해 다음에 올 문자의 분포를 예측합니다.\n",
        "3. 다항 분포 (multinomial distribution)를 사용하여 예측 된 문자의 index를 계산하고, 이 예측 된 문자를 모델에 대한 다음 입력으로 사용합니다.\n",
        "4. 모델에 의해 반환 된 RNN state는 모델에 들어가 이제는 단 하나의 단어가 아닌 더 많은 컨텍스트를 갖게됩니다.\n",
        "다음 단어를 예측 한 후에 수정 된 RNN state가 다시 모델로 들어가 이전에 예측 된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식입니다.\n",
        "\n",
        "<img src=\"https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png\" height=300>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNrqLUOWUO25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      #       predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cK4fV71UfHJ",
        "colab_type": "code",
        "outputId": "c7df03aa-b7ff-476a-8fab-a7946d2889dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: I would sot percuige!\n",
            "Wo all one rithat some-possied:\n",
            "I rave in amls,\n",
            "If thou wilting not against us by a terron in this?\n",
            "\n",
            "LUCENTIO:\n",
            "I have pruseed you to and fallings nigrth\n",
            "To a pity cheer'for feel.\n",
            "\n",
            "CARESBUY:\n",
            "And speak you? for in our lords\n",
            "Syol the parters of the langs on thes and duspicion tursh princome exfosentions;\n",
            "O not some how recoves my we proud,\n",
            "To father me at out soothenter, 'sifum he, he shall bemover soll it\n",
            "Bay, loss? our petitness of him.\n",
            "\n",
            "Servatt:\n",
            "Tell changef, by make ot nave, deceive!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "No, a thou arr: and at hingly fair ane sweet, and stird, we so must\n",
            "converon, loved me well gentle against try lie.\n",
            "\n",
            "CLARENCE:\n",
            "My cursery lif-hourself and roya\n",
            "Nade of yourselves be preserved\n",
            "Take our worshound not to see hear ot a labben?\n",
            "\n",
            "DUKE OF YORK:\n",
            "O, yeatar failer.\n",
            "\n",
            "Farse: But frife am so no tolu.\n",
            "\n",
            "CARIELA:\n",
            "Saye you prinent, or wordom you here stoeldst\n",
            "Is wa'tly we will not fram our comfort\n",
            "save us be ither, no, and away of this rishal\n",
            "Ladenor, shall eppurie h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDPuaVfLVcQJ",
        "colab_type": "text"
      },
      "source": [
        "생성 된 텍스트를 살펴보면 모델이 대문자와 단락을 언제 알 수 있는지를 알 수 있으며 셰익스피어와 유사한 글쓰기 어휘를 모방합니다. 작은 epoch에서도 일관된 문장을 형성하는 것을 조금 배웠습니다. 어떻게 결과를 향상 시킬 수 있을까요? \n",
        "\n",
        "\n",
        "- epoch을 늘린다. EPOCHS=30\n",
        "- start string을 바꾸어 실험한다. \n",
        "- 다른 RNN layer를 추가한다. \n",
        "- 다른 temperature를 시도해본다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKPUzNx3VB72",
        "colab_type": "text"
      },
      "source": [
        "### Customized Training\n",
        "\n",
        "1. RNN state 초기화. tf.keras.Model.reset_states method\n",
        "2. 배치별로 데이터셋을 돌며 예측합니다.\n",
        "3. tf.GradientTape 열어 예측과 손실을 계산합니다.\n",
        "4. tf.GradientTape.grads 메소드를 사용하여 loss gradient를 계산합니다.\n",
        "5. 옵티마이저의 tf.train.Optimizer.apply_gradients 메소드를 사용하여 더 실행합니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4-qa1m0Ugdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JysZKR96VEVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFcvu2RKVFn4",
        "colab_type": "code",
        "outputId": "f7d45126-fbdc-4730-c1e6-e73d95d3ed7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initially hidden is None\n",
        "    hidden = model.reset_states()\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions = model(inp)\n",
        "              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "\n",
        "          grads = tape.gradient(loss, model.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "          if batch_n % 100 == 0:\n",
        "              template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "              print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1757\n",
            "Epoch 1 Batch 100 Loss 2.3452\n",
            "Epoch 1 Loss 2.1657\n",
            "Time taken for 1 epoch 26.30596113204956 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1165\n",
            "Epoch 2 Batch 100 Loss 1.9056\n",
            "Epoch 2 Loss 1.8287\n",
            "Time taken for 1 epoch 27.32648515701294 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7810\n",
            "Epoch 3 Batch 100 Loss 1.6569\n",
            "Epoch 3 Loss 1.6253\n",
            "Time taken for 1 epoch 25.935643434524536 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5970\n",
            "Epoch 4 Batch 100 Loss 1.5132\n",
            "Epoch 4 Loss 1.5052\n",
            "Time taken for 1 epoch 26.1180682182312 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4850\n",
            "Epoch 5 Batch 100 Loss 1.4261\n",
            "Epoch 5 Loss 1.4253\n",
            "Time taken for 1 epoch 26.037880182266235 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4080\n",
            "Epoch 6 Batch 100 Loss 1.3636\n",
            "Epoch 6 Loss 1.3658\n",
            "Time taken for 1 epoch 25.981739044189453 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3486\n",
            "Epoch 7 Batch 100 Loss 1.3130\n",
            "Epoch 7 Loss 1.3154\n",
            "Time taken for 1 epoch 25.879635334014893 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2993\n",
            "Epoch 8 Batch 100 Loss 1.2675\n",
            "Epoch 8 Loss 1.2683\n",
            "Time taken for 1 epoch 25.921844482421875 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2539\n",
            "Epoch 9 Batch 100 Loss 1.2237\n",
            "Epoch 9 Loss 1.2260\n",
            "Time taken for 1 epoch 25.913885354995728 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2146\n",
            "Epoch 10 Batch 100 Loss 1.1827\n",
            "Epoch 10 Loss 1.1817\n",
            "Time taken for 1 epoch 26.02943229675293 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13NYRzc6VGn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}