{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "TIDEH_path = os.path.join('TiDeH-master','TiDeH-master')\n",
    "sys.path.append(TIDEH_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module import\n",
    "from tideh import estimate_parameters\n",
    "from tideh import load_events\n",
    "from tideh import predict\n",
    "filename = os.path.join(TIDEH_path, 'data/example/sample_file.txt')\n",
    "(_, start_time), events = load_events(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional parameters passed to infectious rate function\n",
    "add_params = {'t0': start_time, 'bounds': [(-1, 0.5), (1, 20.)]}\n",
    "obs_time = 48 # observation time of 2 days\n",
    "pred_time = 168 # predict for one week\n",
    "params, err, _ = estimate_parameters(events=events, obs_time=obs_time, **add_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimated parameters are:\")\n",
    "print(\"p0:   %.3f\" % params[0])\n",
    "print(\"r0:   %.3f\" % params[1])\n",
    "print(\"phi0: %.3f\" % params[2])\n",
    "print(\"tm:   %.3f\" % params[3])\n",
    "print(\"Average %% error (estimated to fitted): %.2f\" % (err * 100))\n",
    "\n",
    "# predict future retweets\n",
    "_, total, pred_error = predict(events=events, obs_time=obs_time, pred_time=pred_time, p_max=None, params=params,\n",
    "                               **add_params)\n",
    "\n",
    "print(\"Predicted number of retweets from %s to %s hours: %i\" % (obs_time, pred_time, total))\n",
    "print(\"Predicted number of retweets at hour %s: %i\" % (pred_time,\n",
    "                                                       len([e for e, _ in events if e <= obs_time]) + total))\n",
    "print(\"Prediction error (absolute): %.0f\" % pred_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tick.hawkes import SimuHawkesSumExpKernels, HawkesSumExpKern, ModelHawkesSumExpKernLeastSq, HawkesExpKern\n",
    "from tick.plot import plot_point_process, plot_basis_kernels, plot_hawkes_kernels\n",
    "import itertools\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(TIDEH_path, 'data/example/sample_file.txt')\n",
    "(_, start_time), events = load_events(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for seq in events:\n",
    "    seqs.append(seq[0])\n",
    "seqs = [np.asarray(seqs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HawkesSumExpKern <br>\n",
    "<https://x-datainitiative.github.io/tick/modules/generated/tick.hawkes.HawkesSumExpKern.html?highlight=hawkessumexpkern> <br>\n",
    "<https://x-datainitiative.github.io/tick/auto_examples/plot_hawkes_varying_baseline.html?highlight=hawkessumexpkern>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decays = [0.1]\n",
    "print (datetime.datetime.now())\n",
    "learner = HawkesSumExpKern(decays, penalty='elasticnet', elastic_net_ratio=0.8)\n",
    "learner.fit(seqs)\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattend = list(itertools.chain.from_iterable(seqs))\n",
    "t_min = int(np.ceil(np.min(flattend)))\n",
    "t_max = int(np.floor(np.max(flattend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.plot_estimated_intensity(seqs, t_min=t_min, t_max=t_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hawkes_kernels(learner, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tideh import load_events_vec\n",
    "from tideh import training_cross_validation\n",
    "\n",
    "number_of_files = 100  # number of files to train on\n",
    "file_name_prefix = os.path.join(TIDEH_path, 'data/training/RT')  # file names prefix of files used for training\n",
    "iterations = 5 # number of cross validation iterations\n",
    "pred_time = 168  # prediction time (hours)\n",
    "\n",
    "# get file paths of files to use for training\n",
    "file_names = [file_name_prefix + str(i) + '.txt' for i in range(1, number_of_files + 1)]\n",
    "\n",
    "# load events for optimized training\n",
    "events_data = []\n",
    "for file in file_names:\n",
    "    events_data.append(load_events_vec(file, 1 / 3600, 24))  # convert event_times and start_time to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for seq in events_data:\n",
    "    seqs.append(seq[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HawkesSumExpKern(decays[, penalty, C, â€¦])<br>\n",
    "Hawkes process learner for sum-exponential kernels with fixed and given decays, with many choices of penalization and solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decays = [0.1]\n",
    "print (datetime.datetime.now())\n",
    "learner = HawkesSumExpKern(decays, penalty='elasticnet', elastic_net_ratio=0.8, n_baselines=2, period_length=100)\n",
    "learner.fit(seqs)\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattend = list(itertools.chain.from_iterable(seqs))\n",
    "t_min = int(np.ceil(np.min(flattend)))\n",
    "t_max = int(np.floor(np.max(flattend)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (datetime.datetime.now())\n",
    "learner.plot_estimated_intensity(seqs, t_min=t_min, t_max=t_max)\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_hawkes_kernels(learner, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
