{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is from http://gpss.cc/gpss13/assets/lab1.pdf, https://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/index.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GP는 함수에 대한 prior를 정의합니다.** 이 prior는 관측된 데이터를 통해 posterior로 변환 할 수 있습니다.\n",
    "함수에 대한 분포를 정의하는 것이 어려워 보일 수 있습니다. \n",
    "**하지만 저희가 해야할 것은 우리는 단지 \"유한\"하지만 \"임의의 점\" 집합에서 함수의 값에 대한 distribution을 정의하는 것입니다.**\n",
    "\n",
    "<font color=red>why? how?</font>\n",
    "**GP는 이 임의의 유한한 함수값들이 joint Gaussian distribution(mean function과 covariance function으로 정의된)을 따른다고 가정하기 때문입니다.**\n",
    "\n",
    "<font color=red>**Definition : A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.**</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/basic1.PNG\" alt=\"various_kernel\" title=\"various_kernel\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이해하기 힘드실 수 있으니 무한한 점에서 벗어나 2개의 점을 예시로 들어보겠습니다.\n",
    "만약 위와 같이 **x1과 x2가 특정한 joint distribution을 따른다고 가정한다면?(GP의 가정)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1을 관측한 값, x2를 아직 관찰하지 못한 값이라고 했을 때, 이 joint distribution을 통하여 이론 수업때 배운것과 같이 conditional probability를 계산할 수 있게 됩니다. 따라서 prior와 이 observation을 통하여 posterior까지 구할 수 있게 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/basic2.PNG\" alt=\"various_kernel\" title=\"various_kernel\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 다시 무한한 점, 함수로 돌아와보겠습니다.\n",
    "우리의 GP의 가정에 따라 몇몇 관찰된 아웃풋 f 와 그렇지 않은 점들 f* 는 위와 같이  joint Gaussian distribution을 따릅니다.\n",
    "그렇다면 관측된 f 로부터 conditional distribution을 구할 수 있고, 따라서 \n",
    "f(observation)와 prior로 부터 f* 의 posterior까지 계산할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 K 는 수업시간에 배운 kernel function을 observe 된 x value에 적용하여 observe x_value 끼리의 관계를 나타내는 matrix 입니다. K* 는 observe 된 x와 observe 되지 않은 x 에 kernel function을 적용한 것이고, K** 는 observe 되지 않은 x value에 kernel function을 적용한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean을 고려 후에, GP모델은 covariance function(kernel)에 의해 결정됩니다. kernel은 모델이 어떻게 generalize를 할지, 새로운 데이터를 추정할지 결정합니다. covariance function으로 사용할 수 있는 함수는 다양합니다(이론수업 참조)\n",
    "\n",
    "**covaraince function 을 통해서 observe 된 점들과 가까운 점엔 높은 confidence를, 먼 점엔 낮은 confidence 를 갖게 됩니다.(아래의 회색부분이 confidence 95% 를 나타낸 것)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/prior,posterior.PNG\" alt=\"various_kernel\" title=\"various_kernel\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Tutorial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started : the covariace function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리에게 익숙한 Gaussian distribution은 mean vector와 variance matrix로부터 정의가 됩니다. 이를 일반화하여 생각하면 확장된 개념을 생각해볼 수 있습니다. Gaussian Process는 mean function  과 covariance 함수로부터 정의됩니다. 따라서 Gussian distribution은 벡터들에서 놀지만 Gaussian processs는 함수들에서 놀고 있습니다. 이러한 Gaussian process는 아래와 같이 표현됩니다.\n",
    "\n",
    "\\begin{align}\n",
    "\\ {f} &∼GP(m(x),k(x,x')) \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중요한 커널(covariance function)부터 시작해보겠습니다.\n",
    "이 이론실습에서는 RBF 커널을 중심으로 실습을 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1 # input dimension\n",
    "var = 1. # variance\n",
    "theta = 0.2 # lengthscale\n",
    "k = GPy.kern.RBF(d,var,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the kernel can be obtained using the command <font color=blue> print (k)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with <font color=blue> k.plot()</font> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.plot(x=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>*TRY!</font>\n",
    "### To get an insight on the effect of the parameters their shape, try\n",
    "#### <font color=green>1. Change the lengthscale of RBF kernel and plot!</font>\n",
    "The value of the kernel parameters can be accessed and modified using <font color =blue>k['.*lengthscale']</font> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY in here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Question 1</font>\n",
    "<font color=red>1.What is the effect of the lengthscale parameter ?</font>\n",
    "\n",
    "<font color=red>2.Similarly, change the previous bit of code to see the influence of the variance parameter</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>2. Change the kernel (ex. exponential, linear, periodic exponential) and plot!</font>\n",
    "hint1 :  <font color =blue> \"GPy.kern.then tab button\" </font> will show you several kernel to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY in here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot several kernel, it will be like this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/various_kernel.PNG\" alt=\"various_kernel\" title=\"various_kernel\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations to combine kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine several kernels also with operation ( * or + etc)\n",
    "\n",
    "## <font color=green>*TRY!</font>\n",
    "\n",
    "#### <font color=green onmouseover=\"aa\">1. use * operation with 2 kernels and plot product of kernels</font>\n",
    "hint : <a href=\"https://github.com/upstage/\" title=\"k1 * k2 will work\">MouseOver</a>\n",
    "answer : <a href=\"https://github.com/upstage/\" title=\"# Product of kernels\n",
    "k1 = GPy.kern.RBF(1,1.,2.)\n",
    "k2 = GPy.kern.Matern32(1, 0.5, 0.2)\n",
    "k_prod = k1 *k2\n",
    "display(k_prod)\n",
    "k_prod.plot()\">MouseOver</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try in here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green onmouseover=\"aa\">2. use + operation with 2 kernels and plot product of kernels</font>\n",
    "hint : <a href=\"https://github.com/upstage/\" title=\"k1 + k2 will work\">MouseOver</a>\n",
    "answer : <a href=\"https://github.com/upstage/\" title=\"# Sum of kernels\n",
    "k1 = GPy.kern.RBF(1,1.,2.)\n",
    "k2 = GPy.kern.Matern32(1, 0.5, 0.2)\n",
    "k_add = k1 + k2\n",
    "display(k_add)\n",
    "k_add.plot()\">MouseOver</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try in here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(d) # By default, the parameters are set to 1.\n",
    "theta = np.asarray([0.2,0.5,1.,2.])\n",
    "\n",
    "figure, axes = plt.subplots(2,2, figsize=(8,8), tight_layout=True)\n",
    "\n",
    "for t,a in zip(theta,axes.flatten()):\n",
    "    k['.*lengthscale']=t\n",
    "    k.plot(x=0,ax =a, title = \"[RBF Kernel]lengthscale: \"+str(t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.Exponential(d) # By default, the parameters are set to 1.\n",
    "theta = np.asarray([0.2,0.5,1.,2.])\n",
    "\n",
    "figure, axes = plt.subplots(2,2, figsize=(8,8), tight_layout=True)\n",
    "\n",
    "for t,a in zip(theta,axes.flatten()):\n",
    "    k['.*lengthscale']=t\n",
    "    k.plot(x=0,ax =a, title = \"[RBF Kernel]lengthscale: \"+str(t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sample paths from a Gaussian Process\n",
    "\n",
    "Represents our belief about the function distribution, which we pass through parameters.\n",
    "This will be priors defined by GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Priors with GP using RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) #난수 생성기 초기화 함수,\n",
    "\n",
    "k = GPy.kern.RBF(input_dim=1,lengthscale=0.2,variance=0.1)\n",
    "\n",
    "X_ = np.linspace(0.,1.,500) # 500 points evenly spaced over [0,1]\n",
    "X_ = X_[:,None] # reshape X to make it n*D\n",
    "mu = np.zeros((500)) # vector of the means\n",
    "C = k.K(X_,X_) # covariance matrix\n",
    "\n",
    "# Generate 20 sample path with mean mu and covariance C\n",
    "Y_samples = np.random.multivariate_normal(mu,C,20)\n",
    "\n",
    "plt.title(\"Distribution over function using GP with RBF kernel\")\n",
    "for i in range(20):\n",
    "    plt.plot(X_[:],Y_samples[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample paths are from our prior to function distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution changes in presence of Observed data.\n",
    "\n",
    "We will now see how to create a GP regression model with GPy.\n",
    "We consider the toy function f(x) = − cos(πx) + sin(4πx) over [0, 1]\n",
    "and we assume we have the following\n",
    "observations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate observed data\n",
    "np.random.seed(1) #난수 생성기 초기화 함수,\n",
    "X = np.random.uniform(0.,1.,10)[:,None]\n",
    "Y = -np.cos(np.pi*X) +np.sin(4*np.pi*X) + np.random.randn(10,1)*0.05\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Observed value\")\n",
    "plt.plot(X[:,:1],Y,'kx',mew=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed 10 values. Now what? Then we need to get posterior from this observation and priors!\n",
    "and then see the samples function from calculated posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPy.models.GPRegression(X,Y,k)\n",
    "mu,C =m.predict(X_,full_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_samples = np.random.multivariate_normal(mu.flatten(),C,15)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.title(\"sample paths from GP with RBF kernel\")\n",
    "for i in range(10):\n",
    "    plt.plot(X_[:],Y_samples[i,:])\n",
    "plt.scatter(X,Y, color = \"black\",s=100,zorder=15,label=\"observed\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel의 hyper-parameter가 optimize가 안되었는지 observed 근처 지점에서 높은 confidence가 보이지 않습니다. 현재 모델의 상태를 한번 확인해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot()\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper parameter를 optimize하기 전의 GP regression model을 보여줍니다. 색칠된 곳은 ~95% confidence interval입니다. Observed 된 데이터포인트 주변으로 confidence가 높을것으로 예상했던 것과 달리 큰 차이를 보이지 않습니다. 커널의 hyper-parameter가 이 데이터에는 맞지 않았나 봅니다. 따라서 커널 optimize 함수를 통하여 hyper-parameter를 optimize하면 모델이 어떻게 변화하는지 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강의에서 봤듯이, 최적의 파라미터는 <font color=red>observation의 likelihood를 maximizing하는 값</font>으로 추정합니다. 이 optimization을 실행하기 전에 우리는 variance 중에 하나라도 negative가 되는걸 원치 않기 때문에 모든 파라미터가 positive가 되도록 constrain을 주어야합니다. \"m.constrain_positive('')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.constrain_positive('')\n",
    "m.optimize()\n",
    "m.plot()\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu,C =m.predict(X_,full_cov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_samples = np.random.multivariate_normal(mu.flatten(),C,10)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.title(\"sample paths from GP with RBF kernel\")\n",
    "for i in range(10):\n",
    "    plt.plot(X_[:],Y_samples[i,:])\n",
    "plt.scatter(X,Y, color = \"black\",s=100,zorder=10,label=\"observed\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Question 2</font>\n",
    "<font color=red>3.커널의 hyper-parameter를 optimize하는 방법 외에 GP regression model의 성능을 향상시킬 수 있는 방법은 무엇이 있을까요?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 GP로 정의된 prior, 그 후에 observed 된 데이터를 이용하여 posterior 계산, 마지막으로 optimizing을 통한 hyper-parameter의 optimization을 보았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>TRY section</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순서 : \n",
    "\n",
    "1. mu를 정의하고 kernel을 선택한 후, Prior을 GP로 정의합니다. Prior에서 뽑은 Sample을 뽑아 확인합니다.\n",
    "2. Observed data를 생성합니다.\n",
    "3. Observed data를 통해 posterior를 계산합니다. Posterior에서 뽑은 Sample을 뽑아 확인합니다.\n",
    "\n",
    "(4). 주어진 데이터에 Posterior에서 뽑은 Sample이 fit하지 않는다고 생각되면, Kernel의 parameter를 optimize하거나 다른 kernel을 시도해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prior by GP\n",
    "#### 1. Use Exponential kernel\n",
    "\n",
    "RBF 대신 Exponential kernel을 사용하여 prior를 정의하는 것 외엔 위의 코드와 같습니다. \n",
    "prior를 정의했던 위의 코드를 참고하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try in here!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X를 uniform distribution을 사용하였던 것과 달리 normal distribution을 통하여 생성합니다.\n",
    "그 외에는 위의 코드와 같으니 참고하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. X will be 10 random numbers from normal distribution.\n",
    "# 2. Y will be -np.cos(np.piX) +np.sin(4np.piX) + np.random.randn(10,1)*0.05\n",
    "# 3. Plot Observed data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model with observed data and kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build GP model with X,Y,Exponential kernel\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot GP model before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Plot GP model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If needed, optimize GP model with Observed data. or you can also change kernel function to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Optimize GP model to Observed data.\n",
    "# 2. Plot Fitted GP model\n",
    "\n",
    "\n",
    "# or You can change the kernel and see if improved!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Option)3-2 Gaussian Process Regression model (2D example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inputs and outputs\n",
    "np.random.seed(10) #난수 생성기 초기화 함수,  시험이나 기능 테스트를 수행할 때 수치는 취득하고 싶으나 재현성이 필요할때 시드를 고정하여 난수를 생성.\n",
    "#X = np.linspace(0.05,0.95,10)[:,None]\n",
    "X = np.random.uniform(-3.,3.,(100,2))\n",
    "#X = np.random.uniform(-3.,3.,(50,2))\n",
    "Y = np.sin(X[:,0:1]) * np.sin(X[:,1:2])+np.random.randn(100,1)*0.05\n",
    "plt.plot(X,Y,'kx',mew=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel\n",
    "ker = GPy.kern.Matern52(2,ARD=True) + GPy.kern.White(2)\n",
    "\n",
    "# create simple GP model\n",
    "m = GPy.models.GPRegression(X,Y,ker)\n",
    "display(m)\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize and plot\n",
    "m.optimize(messages=True,max_f_eval = 1000)\n",
    "display(m)\n",
    "m.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaist",
   "language": "python",
   "name": "kais"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
